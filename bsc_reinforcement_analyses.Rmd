---
title: "bsc_reinforcement_analyses"
author: "Shoshana Rapley"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup}
knitr::opts_chunk$set(echo = TRUE)

# Packages
pacman::p_load(adehabitatHR, amt, asnipe, atlastools, beepr, emmeans, ggfortify, ggmap, ggnewscale, ggraph, ggpubr, ggridges, glmmTMB, gtools, igraph, janitor, jtools, lme4, lmerTest, move2, momentuHMM, readxl, scattermore, sf, sp, STRAND, suncalc, survival, survminer, terra, tidygraph, tidyterra, tidyverse, wildlifeDI)

# Google API key for ggmaps
ggmap::register_google(key = readChar("apikey_google.txt", nchars = file.info("apikey_google.txt")$size))

# Background map MR zones 1 and 2
map_z14 <- get_map(c(144.4380, -37.9000), zoom=14, maptype = "satellite")
map_z15 <- get_map(c(144.4380, -37.9000), zoom=15, maptype = "satellite")

# Metadata - translocation information
metadata  <- read.csv("data/metadata.csv") %>%
  clean_names() %>%
  rename(id = identity,
         mortality = mortality_or_capture) %>%
  mutate(start_date = as_date(dmy(start_date)),
         end_date = as_date(dmy(end_date)),
         mortality = as_date(dmy(mortality)))
```

# Introduction

Reinforcement is a form of conservation translocation used to stabilise or enhance populations. Reinforcement is also a stage within other forms of translocation, which usually involve multiple release cohorts and therefore have cohorts released into the presence of conspecifics. Even a reintroduction project will have conspecifics present from the second release. Benefits of reinforcing release are assumed but rarely empirically tested â€“ and if they are its usually from perspective of reinforcing individuals, not the population as a whole. Outcomes of reinforcement are mediated by social interactions, which are also assumed but not often empirically tested. 

Here we test the outcomes of reinforcement for a population of bush stone-curlew where all individuals in the population (resident and reinforcing) are accounted for and tracked with GPS. We ask: do the cohorts integrate? Do social interactions benefit the reinforcing cohort? And, do the residents change their behaviour?

## Translocation

We translocated 35 adult captive-bred bush stone-curlews from Mt Rothwell captive colony to Mt Rothwell Zone 1 (fenced sanctuary) in two stages. The first (pilot) cohort of 16 birds was released between October 2022 and June 2023. The second (reinforcing) cohort of 20 birds was released in June 2023. One bird in the first cohort was taken back into captivity after a single day and re-released in the second cohort; hence total 35 individuals.

## Telemetry

All translocated birds were fitted with a GPS tracker (Ornitrak20 from Ornitela) with a duty cycle of a fix every 60 seconds (or reduced when battery low). Telemetry data were stored on Movebank. We collected data from the release date of each individual until the 12th of January 2024 (when GPS devices were removed from all remaining birds except one, ahead of the 3G shutdown).

The study period for this analysis ends 5th August 2023, when half of the reinforcing cohort were translocated to a secondary site (Orana).

# Data cleaning

High throughput animal tracking data require filtering to remove erroneous points, while maintaining real movement data. We follow the workflow by [Gupte et al. (2021)](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/1365-2656.13610), that is:

1) temporal filtering
2) filtering by quality covariates
3) filtering biologically unrealistic movement
4) median smoothing

## Test pipeline

We first tested the pipeline with a subset of the data (birds "Marmalade" & "Fauna"). Marmalade left the fenced area during the study while Fauna did not. 

### 1) Temporal filtering

We removed points after the end of the study period (5/8/23 at the start of next experiment - translocation to Orana).

```{r}
# import data from movebank
data_raw <- readr::read_csv("movebank/Fauna01.csv", show_col_types = FALSE) %>%
  rbind(readr::read_csv("movebank/Marmalade01.csv", show_col_types = FALSE)) %>%
  clean_names() %>%
  # Time in posix format
  mutate(datetime = as.POSIXct(study_local_timestamp, "%Y-%m-%d %H:%M:%S"),
         date = as.Date(datetime)) %>%
  # Remove days after end of tracking period
  filter(date < "2023-08-04") %>%
  rename(id = individual_local_identifier)

# plot raw data
ggmap(map_z13)+
  geom_path(data=data_raw, aes(location_long, location_lat), 
            colour = "yellow", alpha = .6)+
  geom_point(data=data_raw, aes(location_long, location_lat), 
             colour = "yellow", alpha = .6)+
  theme_void()+
  facet_wrap(~id)
```

### 2) Filtering by quality covariates

We filter using the following covariates: 

* satellite count
* horizontal dilution of precision (HDOP)
* altitude

In the past I've found filtering by satellite count and hdop can remove errors, but when applied with the wrong parameters they can have a high rate of false positives (removing real movement) and low rate of true positives (removing unrealistic movement) so we don't want to be overly aggressive with the application of these filters.

```{r}
# Histogram of satellite vales
hist(data_raw$gps_satellite_count)

# Plot track, colour by satellite values
ggmap(map_z13)+
  geom_path(data=data_raw, aes(location_long, location_lat), colour = "white")+
  geom_point(data=data_raw, aes(location_long, location_lat, colour = gps_satellite_count))+
  scale_colour_viridis_c()+
  theme_void()+
  facet_wrap(~id)
```

Most points have a satellite count >=4. Not many of the obvious spikes have low satellite count. We'll filter to include only satellite count >=4. 

Now look at the horizontal dilution of precision (HDOP).

```{r}
# Histogram of hdop vales
hist(data_raw$gps_hdop)

# Plot track, colour by hdop values
ggmap(map_z13)+
  geom_path(data=data_raw, aes(location_long, location_lat), colour = "white")+
  geom_point(data=data_raw, aes(location_long, location_lat, colour = gps_hdop))+
  scale_colour_viridis_c()+
  theme_void()+
  facet_wrap(~id)
```

There are a handful of massive hdop values (5-15) but the vast majority are =<2. Again the obvious spikes don't have high hdop values. We'll filter to only include hdop <=2. 

Next we'll apply filtering on the basis of altitude. 

Incorrect GPS fixes often have incorrect altitude. High altitude was not expected as no birds were undertaking long-distance flight, which is the only time high altitude is possible. All birds were wing-clipped on release and some later moulted and undertook short-distance flight, but not at high altitude. 

First we need to find flight height altitude by correcting for ground elevation. We are using the [FABDEM (Forest And Buildings removed Copernicus 30m DEM)](https://gee-community-catalog.org/projects/fabdem/).

```{r}
# read in FABDEM
fabdem <- rast("data/S38E144_FABDEM_V1-0.tif") 

# convert data to spatial points
coords <- data_raw %>%
  vect(geom = c("location_long", "location_lat"), crs = "EPSG:4326")

# Extract dem and add to dataframe
data_alt <- data_raw %>%
  mutate(terra::extract(fabdem, coords, ID = FALSE)) %>%
  rename(elevation = "S38E144_FABDEM_V1-0") %>%
  # calculate flight height
  mutate(altitude = height_above_msl - elevation)

# Histogram altitude
hist(data_alt$altitude)

# Summary statistics altitude
summary(data_alt$altitude)
quantile(data_alt$altitude, probs = 0.95)

# Plot track, colour by altitude values
ggmap(map_z13)+
  geom_path(data=data_alt, aes(location_long, location_lat), colour = "white")+
  geom_point(data=data_alt, aes(location_long, location_lat, colour = altitude))+
  scale_colour_viridis_c()+
  theme_void()+
  facet_wrap(~id)
```

The median altitude (above the ground surface) was 9m. The min (-2060m) and max (8313m) altitude were considered unrealistic. Many of the obvious spikes have very high or negative altitude. 

We cut off the maximum altitude at 60 (just above the 0.95 quartile) and the minimum to -10m (ground level allowing for some error).

```{r}
# Apply quality covariate filters
data_qfilt <- data_alt %>%
  filter(gps_satellite_count >= 4) %>%
  filter(gps_hdop <=2) %>%
  filter(altitude < 60 & altitude >-10)

# Plot effect of filtering
ggmap(map_z13)+
  geom_path(data=data_raw, aes(location_long, location_lat), 
            colour = "purple", alpha = .6)+
  geom_point(data=data_raw, aes(location_long, location_lat), 
             colour = "yellow")+
  geom_point(data=data_qfilt, aes(location_long, location_lat), 
             colour = "purple")+
  theme_void()+
  facet_wrap(~id)
```

This successfully filtered out most of the obvious spikes.

### 3) Filtering biologically unrealistic movement

To remove spikes in the data we can filter out positions with extreme incoming and outgoing speeds. First we need to define biologically realistic incoming and outgoing speeds.

```{r}
# Append turning angle, incoming/outgoing speeds to data frame per bird
birds <- unique(data_raw$id)

data_speed <- data.frame()

for(i in 1:length(birds)){
 subset <- filter(data_qfilt, id == birds[i])

 temp <- subset %>%
   mutate(speed_in  = atl_get_speed( 
           data = subset, x = "utm_easting", y = "utm_northing", time = "datetime", type = c("in")),
    speed_out = atl_get_speed(
           data = subset, x = "utm_easting", y = "utm_northing", time = "datetime", type = c("out")),
    angle = atl_turning_angle(
           data = subset, x = "utm_easting", y = "utm_northing", time = "datetime"),
    speed_delta = abs(speed_in - speed_out)
    )
 
 data_speed <- rbind(data_speed, temp)
}

# Histogram incoming speeds
hist(data_speed$speed_in)

# Summary statistics incoming speeds
summary(data_speed$speed_in)
quantile(data_speed$speed_in, probs = 0.99, na.rm = TRUE)

# Histogram outgoing speeds
hist(data_speed$speed_out)

# Summary statistics outgoing speeds
summary(data_speed$speed_out)
quantile(data_speed$speed_out, probs = 0.95, na.rm = TRUE)

# Histogram turning angle
hist(data_speed$angle)

# Summary statistics turning angle
summary(data_speed$angle)

# Histogram difference between incoming and outgoing speed
hist(data_speed$speed_delta)

# Summary statistics difference between incoming and outgoing speed
summary(data_speed$speed_delta)
quantile(data_speed$speed_delta, probs = 0.95, na.rm = TRUE)

# Plot track, colour by speed
ggmap(map_z14)+
  geom_path(data=data_speed, aes(location_long, location_lat), colour = "white")+
  geom_point(data=data_speed, aes(location_long, location_lat, colour = speed_delta))+
  scale_colour_viridis_c()+
  theme_void()+
  facet_wrap(~id)
```

Speeds were usually <2m/s and outgoing speeds while turning are likely <1m/s. A sharp turning angle was defined as <90 degrees. Sharp increase/decrease in speed also isn't expected, with the vast majority of difference between incoming and outgoing speed 0.1m/s.

We defined biologically realistic movement as speeds <2m/s (a note of caution: this shouldn't be used when the birds are undertaking long distance movements as faster speeds may be possible e.g. while gliding on wind), turning speeds of 0.5m/s, and delta speed as 0.4m/s. 

```{r}
# Apply turning angle filtering
data_sfilt <- data_speed %>%
  filter(speed_in < 2) %>%
  filter(speed_delta < 0.4) %>%
  filter(!(speed_out >0.5 & angle <90))

# Plot effect of filtering
ggmap(map_z14)+
  geom_path(data=data_raw, aes(location_long, location_lat), 
            colour = "purple", alpha = .6)+
  geom_point(data=data_raw, aes(location_long, location_lat), 
             colour = "yellow")+
  geom_point(data=data_qfilt, aes(location_long, location_lat), 
             colour = "orange")+
  geom_point(data=data_sfilt, aes(location_long, location_lat), 
             colour = "purple")+
  theme_void()+
  facet_wrap(~id)
```

### 4) Median smoothing

Even after speed/angle filtering, we retain some smaller-scale jitter. These are challenging to remove as they lie within the bounds of realistic movement. Median resampling is a method of smoothing the track to reduce jitter. We apply it sparingly, because an overly aggressive approach will cut down on real track tortuousity. 

```{r}
# Apply median smooth by bird
data_smooth <- data.frame()

for(i in 1:length(birds)){
  
  subset <- filter(data_sfilt, id == birds[i])
  
  temp <- atl_median_smooth(data = subset, x = "location_lat", y = "location_long",
                                  time = "datetime", moving_window = 3)
  
  data_smooth <- rbind(data_smooth, temp)
}

# Plot effect of smoothing
ggmap(map_z13)+
  geom_path(data=data_raw, aes(location_long, location_lat), 
            colour = "yellow", alpha = .6)+
  geom_path(data=data_smooth, aes(location_long, location_lat), 
            colour = "purple", alpha = .7)+
  facet_wrap(~id)

# And zoom in to the fenced area
ggmap(map_z15)+
  geom_path(data=data_raw, aes(location_long, location_lat), 
            colour = "yellow", alpha = .6)+
  geom_path(data=data_smooth, aes(location_long, location_lat), 
            colour = "purple", alpha = .7)+
  facet_wrap(~id)

# And visualise without filtered out data
ggmap(map_z15)+
    geom_path(data=data_smooth, aes(location_long, location_lat), 
            colour = "purple", alpha = .7)+
  facet_wrap(~id)
```

*_Visual check using fence polygon._*
We know Fauna didn't leave the study site during the tracking period so this is a good way to check if filtering was successful. 

First define the fenced area.

```{r}
# Define fence polygons (zones 1 and 2)
mtr <- rbind(
  ## zone 1
  c(-37.897319, 144.429048), # S end of NW diagonal
  c(-37.894066, 144.432334), # N end of NW diagonal, i.e. NW corner
  c(-37.894749, 144.438305), # bend at main gate 
  c(-37.894693, 144.438324), # main gate
  c(-37.894803, 144.439214), # bend before N Z1/2 gate
  c(-37.894718, 144.439337), # N Z1/2 gate, i.e. NE corner 
  ## zone 2
  c(-37.892433, 144.440236), # N boundary internal aviary/Z2 i.e. NW corner
  c(-37.892803, 144.443374), # Z2 northern boundary bend 1
  c(-37.893634, 144.444076), # Z2 northern boundary bend 2
  c(-37.894585, 144.447739), # N end of Z2/Z3 boundary, i.e. NE corner
  c(-37.896515, 144.446789), # Z2/Z3 boundary bend 1
  c(-37.896726, 144.446099), # Z2/Z3 boundary bend 2
  c(-37.897964, 144.445706), # Z2/Z3 boundary bend 3
  c(-37.899960, 144.444517), # S end of Z2/Z3 boundary, i.e. SE corner
  c(-37.899908, 144.444288), # Z2/btrw NE corner
  c(-37.898280, 144.440662), # Z1/btrw pen NW corner
  ## zone 1
  c(-37.900692, 144.440368), # Z1/btrw SW corner
  c(-37.902610, 144.443002), # Z1/btrw SE corner
  c(-37.909538, 144.439648), # Z1 SE corner
  c(-37.908999, 144.434965), # Z1 southern boundary bend 1
  c(-37.907537, 144.433359), # Z1 southern boundary bend 2
  c(-37.905958, 144.430140), # Z1 southern boundary bend 3
  c(-37.905486, 144.429479), # Z1 southern boundary bend 4
  c(-37.904649, 144.427566), # Z1 SW corner
  c(-37.897319, 144.429048)  # S end of NW diagonal
  ) %>%  vect(type = "polygons", crs = "EPSG:4326") %>% t() %>%
  # add a buffer for GPS accuracy of 30m
  buffer(30)
```

Then filter for points from "Fauna" outside of the polygon. 

```{r}
# Convert df to points
points <- data_smooth %>%
  filter(id=="Fauna") %>%
  vect(geom = c("location_long", "location_lat"), crs = "EPSG:4326")

# Find points outside polygons
outside <- points[!relate(points, mtr, "intersects")] %>%
  as.data.frame(geom = "XY")
  
# Plot points outside polygon 
ggmap(map_z15)+
  geom_point(data=outside, aes(x, y), colour = "yellow", alpha = 0.7, size = 3)+
  geom_spatvector(data=mtr, inherit.aes = FALSE, colour = "white", fill = NA)+
  scale_colour_viridis_c()+
  theme_void()
```

There are only 65 points outside the fence (given a 30m buffer) which is within tolerance. Interestingly they are all clustered on the southern and eastern fenceline, perhaps there was something about the topography or vegetation in that area that pronounced the errors.

Based on this test subset, I am happy with the filtering parameters for this location.

## Apply filtering 

We apply all of the above steps to the full dataset. 

### 1) Temporal filtering

We removed points after the end of the study period (5/8/23 at the start of next experiment - translocation to Orana).

```{r}
# Import data from movebank
data_raw <- readr::read_csv(fs::dir_ls(path = "movebank")) %>%
  clean_names() %>%
  # Time in posix format
  mutate(datetime = as.POSIXct(study_local_timestamp, "%Y-%m-%d %H:%M:%S"),
         date = as.Date(datetime)) %>%
  # Remove days after end of study
  filter(date < "2023-08-04") %>%
  rename(id = individual_local_identifier)
```

### 2) Filtering by quality covariates

We used the following quality covariates:

* Altitude: set to a minimum of -10m (ground level allowing for some error) and a maximum of 60 (just above the 0.95 quartile on test data)
* HDOP: set to a maximum of 2
* Satellite count: set to a minimum of 4

```{r}
# read in FABDEM
fabdem <- rast("data/S38E144_FABDEM_V1-0.tif") 

# convert data to spatial points
coords <- data_raw %>%
  vect(geom = c("location_long", "location_lat"), crs = "EPSG:4326")

# Extract dem and add to dataframe
data_alt <- data_raw %>%
  mutate(terra::extract(fabdem, coords, ID = FALSE)) %>%
  rename(elevation = "S38E144_FABDEM_V1-0") %>%
  # calculate flight height
  mutate(altitude = height_above_msl - elevation)

# Apply quality covariate filters
data_qfilt <- data_alt %>%
  filter(gps_satellite_count >= 4) %>%
  filter(gps_hdop <=2) %>%
  filter(altitude < 60 & altitude >-10)

# Filtering effect
print(paste("Percentage original data removed: ", round(((nrow(data_raw)-nrow(data_qfilt)) / nrow(data_raw))*100), "%", sep = ""))
```

Percentage original data removed: 28%

### 3) Filtering biologically unrealistic movement

We defined (from test data) biologically realistic movement as speeds <2m/s (a note of caution: this shouldn't be used when the birds are undertaking long distance movements as faster speeds may be possible e.g. while gliding on wind), turning speeds of 0.5m/s, and delta speed as 0.4m/s. 

```{r}
# Append turning angle, incoming/outgoing/delta speeds to data frame per bird
birds <- unique(data_raw$id)

data_speed <- data.frame()

for(i in 1:length(birds)){
 subset <- filter(data_qfilt, id == birds[i])

 temp <- subset %>%
   mutate(speed_in  = atl_get_speed( 
           data = subset, x = "utm_easting", y = "utm_northing", time = "datetime", type = c("in")),
    speed_out = atl_get_speed(
           data = subset, x = "utm_easting", y = "utm_northing", time = "datetime", type = c("out")),
    angle = atl_turning_angle(
           data = subset, x = "utm_easting", y = "utm_northing", time = "datetime"),
    speed_delta = abs(speed_in - speed_out)
    )
 
 data_speed <- rbind(data_speed, temp)
}

# Apply turning angle and speed filtering
data_sfilt <- data_speed %>%
  filter(speed_in < 2) %>%
  filter(speed_delta < 0.4) %>%
  filter(!(speed_out >0.5 & angle <90))

# Filtering effect
print(paste("Percentage original data removed: ", round(((nrow(data_qfilt)-nrow(data_sfilt)) / nrow(data_qfilt))*100), "%", sep = ""))
```

Percentage original data removed: 7%

### 4) Median smoothing

To remove small-scale jitter. We used the smallest possible moving window (3) to retain as much real movement as possible. 

```{r}
# Apply median smooth by bird
data_smooth <- data.frame()

for(i in 1:length(birds)){
  
  subset <- filter(data_sfilt, id == birds[i])
  
  temp <- atl_median_smooth(data = subset, x = "location_lat", y = "location_long",
                                  time = "datetime", moving_window = 3)
  
  data_smooth <- rbind(data_smooth, temp)
}

# Plot smoothed data
ggmap(map_z14)+
  geom_path(data=data_smooth, aes(location_long, location_lat, colour = id), alpha = .7)+
  scale_colour_viridis_d()+
  theme_void()
```

Dropping unneeded columns to reduce the size of the saved file. Originally I kept the utm easting and northing columns from movebank but I noticed a mismatch between the cleaned lat/lon and the utm, so have elected to transform the data when required instead. 

```{r}
# Convert data to spatial and reproject in utm
points <- data_smooth %>%
  vect(geom = c("location_long", "location_lat"), crs = "EPSG:4326") %>%
  # re-project into zone 55S
  project("EPSG:32755") %>%
  st_as_sf()

# allocate utm coords
coords <- st_coordinates(points$geometry)

# format data frame for saving to disk
data <- data_smooth %>%
  # select columns to keep
  dplyr::select(c("id", "datetime", "timestamp", "location_long", "location_lat", "acceleration_raw_x", "acceleration_raw_y", "acceleration_raw_z", "external_temperature", "altitude")) %>%
  # rename columns
  rename(longitude = location_long,
         latitude = location_lat,
         time_local = datetime,
         time_utc = timestamp) %>%
  # add utm xy columns 
  mutate(easting = st_coordinates(points$geometry)[,1],
         northing = st_coordinates(points$geometry)[,2]) %>%
  # add cohort metadata
  left_join(dplyr::select(metadata, c("id","cohort")))

# save to disk
write.csv(data, "data/data_cleaned.csv", row.names = FALSE)
```

# Movement analysis

Here we calculate PRBM metrics from tracking data to assess the establishment of reinforcers. The reinforcement period spans the 55 days between 11 June 2023 (last reinforcer released) and the 5th August 2023 (end of study period when reinforcers are translocated to Orana).

We also compare the response of the residents pre- and post-reinforcement to see if the reinforcement changes the behaviour of the residents. The pre-reinforcement period spans the 142 days between 16th January 2023 (last resident released) to 7th June 2023 (first reinforcer released).

I run these for all the dates to make it simpler (one dataframe instead of multiple per metric for each time period) and then filter to dates as required.

```{r}
# Read in cleaned data 
data <- read.csv("data/data_cleaned.csv") %>%
  # Time in posix format
  mutate(time_local = as.POSIXct(time_local, format = "%Y-%m-%d %H:%M:%OS", 
                                 tz = "Australia/Melbourne"),
         time_utc = as.POSIXct(time_utc, format = "%Y-%m-%d %H:%M:%OS", 
                                 tz = "UTC")) %>%
  # Add date
  mutate(date = as_date(time_local, tz = "Australia/Melbourne"))

# Plot cleaned data to check
ggmap(map_z14)+
  geom_path(data=data, aes(longitude, latitude, colour = id), alpha = .7)+
  scale_colour_viridis_d()+
  theme_void()
```

Calculate basic summary statistics. 

```{r}
# Summary statistic: number of tracked days
length(unique(data$date))

# Summary statistic: total number of tracked days per bird
print(data %>%
  group_by(id) %>%
  summarise(length(unique(date))), n=35)

print(data %>%
  group_by(id) %>%
  summarise(days = length(unique(date))) %>%
  summarise(birddays = sum(days)))
```

Birds were tracked for a total of 284 days between October 2022 and August 2023 for a total of 3533 tracking days (sum of each bird's tracking duration). 

Wobbles and Star tracked for less than two weeks and excluded from further analyses. 

```{r}
data <- data %>%
  filter(!id %in% c("Wobbles", "Star"))
```

## Day/night

Rather than splitting data on calendar days (because they are nocturnal and movement continues over midnight) we want to split the data by diurnal and nocturnal movement. We use the sunrise/sunset time (from suncalc) to add bird date to the data. This also provides additional filtering, because jitter while the bird is stationary at its roost overinflates movement estimates, so counting only noctunal movement gives a better estimate of real movement. Additionally, we add "bird date" to the data, a 24-hour period commencing at sunset (a better indication of a "day" from the bird's perspective than calendar day), so that movements over a night (crossing midnight) can be allocated to the correct grouping. 

```{r}
# Calculate if time is pre/post dawn/dusk
suntime <- getSunlightTimes(date = unique(data$date),
                            lat = -37.90,
                            lon = 144.43,
                            keep = c("sunrise", "sunset"),
                            tz = "Australia/Melbourne") %>%
  subset(dplyr::select = -c(lat, lon)) 

# Append to data frame
data_sun <- left_join(data, suntime) %>%
  mutate(tod = ifelse(time_local>sunrise & time_local<sunset, "day", "night")) %>%
  relocate(time_local, .after = tod) %>%
  na.omit()

# Plot to check - using scattermore to speed up display
ggplot(data_sun)+
  geom_scattermore(aes(easting, northing, colour = tod), alpha = 0.6)+
  coord_sf()+
  theme_void()

# Add "bird date" 
data_sun <- data_sun %>%
  # add column for how long past/to sunset
  mutate(suntime = as.numeric(difftime(time_local, sunset, units = "hours"))) %>%
  # negative sun-time values indicate it's the next day - 
  # therefore allocate previous day calendar date as "bird date"
  mutate(date_bird = as_date(ifelse(suntime > 0, date, date - 1)))

# Save day data
data_day <- data_sun %>%
  filter(tod == "day")

write.csv(data_day, "data/data_clean_day.csv", row.names = FALSE)

# Save night data
data_night <- data_sun %>%
  filter(tod == "night")

write.csv(data_night, "data/data_clean_night.csv", row.names = FALSE)
```

## Distance moved

How long does it take birds to settle? Do acclimation periods differ between cohorts?

Distance moved per night over time. Calculation is done per bird per day (bird date).

Test with one bird, "Fauna". 

```{r}
# read in night data
data_night <- read.csv("data/data_clean_night.csv") %>%
  # format time as posixct
  mutate(time_local = as.POSIXct(time_local))

# format as amt and calculate step lengths
steps <- data_night %>%
  filter(id=="Fauna") %>%
  # format as amt track - add columns as needed
  make_track(.x = easting, .y = northing, .t = time_local, id = id, date_bird = date_bird) %>%
  steps(keep_cols = "start")

# summarise per bird date
summary <- steps %>%
  group_by(date_bird) %>%
  summarise(daily_dist = sum(sl_))

# plot histogram
hist(summary$daily_dist)

# plot over time
ggplot(summary, aes(date_bird, daily_dist))+
  geom_point()+
  geom_smooth(method = "lm")+
  theme_void()
```

Apply to all birds.

```{r}
# for loop to calculate distance moved per bird date
birds <- unique(data_night$id)

distance_daily <- data.frame()

for(i in 1:length(birds)){
  # Subset to bird, convert data to spatial and reproject in utm
    points <- data_night %>%
    # subset to bird
    filter(id==birds[i]) %>%
    vect(geom = c("longitude", "latitude"), crs = "EPSG:4326") %>%
    # re-project into zone 55S
    project("EPSG:32755") %>%
    st_as_sf()

  # save coords
  coords <- st_coordinates(points$geometry)

  # format as amt and calculate step lengths
  steps <- points %>%
    # get utm xy columns 
    mutate(easting = st_coordinates(points$geometry)[,1],
         northing = st_coordinates(points$geometry)[,2]) %>%
    # format as amt track 
    make_track(.x = easting, .y = northing, .t = time_local, id = id, 
               # add additional columns as needed
               date_bird = date_bird) %>%
    steps(keep_cols = "start")

  # summarise per bird date
  summary <- steps %>%
    group_by(date_bird) %>%
    summarise(daily_dist = sum(sl_)) %>%
    mutate(id = birds[i])
  
  # write out
  distance_daily <- rbind(distance_daily, summary)
  
  # alert me
  print(paste("finished calculation for ", birds[i], sep = ""))
}

# save output
write.csv(distance_daily, "results/daily_distance_moved.csv", row.names = FALSE)
```

Plot distance moved as a density plot and movement over time, per bird.

```{r}
# plot histogram
hist(distance_daily$daily_dist)

# plot density by bird
ggplot(distance_daily)+
  geom_density_ridges(aes(daily_dist, id, fill = id), alpha = 0.7)+
  scale_fill_viridis_d()+
  theme_bw()+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+
  theme(legend.position = "none")+
  xlim(0,7000)+
  scale_y_discrete(expand = expansion(add = c(0, 2)))+
  xlab("Distance moved daily (m)") + 
  theme(axis.title.y=element_blank())
```

## Roost establishment

Look at the same idea as the previous section (a metric of establishment) but with diurnal roost movement rather than nightly distanced moved. 

First calculate roost locations. Test with one bird.

```{r}
# read in day data
data_day <- read.csv("data/data_clean_day.csv")

# filter to test bird
test <- data_day %>%
  filter(id == "Aurora")

# plot roosts coloured by date
ggmap(map_z15)+
  geom_density2d(data=test, aes(longitude, latitude, colour = date), inherit.aes = FALSE)+
  theme_void()+
  scale_colour_viridis_d()+
  theme(legend.position = "none")

# plot a series of dates to see if multiple roosts occur
dates <- seq(as.Date("2023-01-01"), as.Date("2023-01-12"), "days")
span <- filter(test, date %in% dates)

ggmap(map_z15)+
  geom_density2d(data=test, aes(longitude, latitude), colour = "yellow", inherit.aes = FALSE)+
  theme_void()+
  facet_wrap(~date)
```

On visual inspection, the vast majority of dates have a single roost. 

Approximate roosts positions using kmeans. Apply to all birds

```{r}
# locate kmeans center for a roost per bird per day
birds <- unique(data_day$id)
days <- unique(data_day$date)

roosts <- data.frame()

for (i in 1:length(birds)){
  for (j in 1:length(days)){
    
    #subset by bird and date
    subset <- subset(data_day, id == birds[i] & date == days[j],
                    dplyr::select= c(easting, northing))
    # skip days where not enough points
      if (length(subset$easting)<4){next}
    
    # find kmean centers
    kmean <- kmeans(subset, centers=1)
    
    # compile data
    out <- as.data.frame(cbind(kmean$centers, npoints = kmean$size)) %>%
      mutate(id = birds[i], date = days[j])
    
    roosts <- rbind(roosts, out)
    
  }
  print(paste("Finished calculation for ", birds[i], sep = ""))
}

# characteristics of cluster data
hist(roosts$npoints)
summary(roosts$npoints)

# plot roosts by number of points
ggplot(roosts)+
  geom_point(aes(easting, northing, size = npoints, colour = id))+
  geom_path(aes(easting, northing, group = id), alpha = 0.3)+
  coord_sf()+
  scale_colour_viridis_d()+
  theme_bw()

# plot for a single bird with date as colour
ggplot(data = filter(roosts, id == "Marmalade"))+
  geom_point(aes(easting, northing, size = npoints, colour = as.Date(date)))+
  geom_path(aes(easting, northing, group = id), alpha = 0.3)+
  coord_sf()+
  scale_colour_viridis_c()+
  theme_bw()+
  theme(axis.line=element_blank(),
      axis.text.x=element_blank(),
      axis.text.y=element_blank(),
      axis.ticks=element_blank(),
      axis.title.x=element_blank(),
      axis.title.y=element_blank())

# save daily roost locations
write.csv(roosts, "results/daily_roost_location.csv", row.names = FALSE)
```

How far between consecutive roosts? 

```{r}
# read in roost locations, format as spatial
roosts <- read.csv("data/daily_roost_location.csv") %>%
  dplyr::select(!"npoints") %>%
  vect(geom = c("easting", "northing"), crs = "EPSG:32755") %>%
  st_as_sf()

# calculate distance between successive roosts per bird
birds <- unique(roosts$id)
distance_roosts <- data.frame()

for(i in 1:length(birds)){
  
  # subset to individual
  points <- filter(roosts, id==birds[i]) %>%
    # date as posix
    mutate(date = as.POSIXct(date, format = "%Y-%m-%d")) %>%
    # ensure arranged by date
    arrange(date) %>%
    #add lag column for geometry comparison
    mutate(previous = lag(geometry))
  
  # allocate the release location as the first location
  release <- st_sfc(st_point(c(274423.45, 5801912.85) ), crs = 32755) %>%
    st_as_sf()
  
  points$previous[1] <- release$x
  
  # calculate the distance between successive points
  out <- points %>%
    mutate(distance_previous = 
             as.numeric(st_distance(points$geometry, points$previous, by_element = TRUE))) %>%
    dplyr::select(!"previous")
  
  # write out
  distance_roosts <- rbind(distance_roosts, out)
}

# hist
hist(distance_roosts$distance_previous)

# transform to latlon
distance_roosts_latlon <- distance_roosts %>%
  vect() %>%
  project("EPSG:4326") %>%
  st_as_sf() %>%
  mutate(date = as_date(date))

# plot roosts with colour as distance from previous
ggplot()+
  geom_sf(data = distance_roosts_latlon, aes(colour = distance_previous))+
  scale_colour_viridis_c()+ 
  theme_void()

# plot roosts with colour as date
ggmap(map_z15)+
  geom_sf(data = distance_roosts_latlon, aes(colour = date), inherit.aes = FALSE)+
  scale_colour_viridis_c(trans = "date")+
  theme_void()
```

There is a day Nutmeg seems to have spent 3km away from their previous roost. IS that true? Investigate this outlier.

```{r}
# Read in cleaned data 
nutmeg <- read.csv("data/data_cleaned.csv") %>%
  # select only Nutmeg
  filter(id == "Nutmeg") %>%
  # Time in posix format
  mutate(time_local = as.POSIXct(time_local, format = "%Y-%m-%d %H:%M:%OS", 
                                 tz = "Australia/Melbourne"),
         time_utc = as.POSIXct(time_utc, format = "%Y-%m-%d %H:%M:%OS", 
                                 tz = "UTC")) %>%
  # Add date
  mutate(date = as_date(time_local, tz = "Australia/Melbourne")) 

# Plot
ggmap(map_z13)+
  geom_point(data=nutmeg, aes(longitude, latitude, colour = date), alpha = 0.8)+
  scale_colour_viridis_c()+
  geom_path(data=nutmeg, aes(longitude, latitude), colour = "white", alpha = 0.4)+
  theme_void()
```

Yep that looks true! On 5-11-23, Nutmeg goes on a flight to the west of the sanctuary, stays over the day near the quarry, then returns to the sanctuary the next day. 

Points outside of fence?

```{r}
# Convert df to points
points <- vect(distance_roosts_latlon)

# Find points outside polygons
outside <- points[!relate(points, mtr, "intersects")] %>%
  as.data.frame(geom = "XY")
  
# Plot points outside polygon 
ggmap(map_z15)+
  geom_point(data=outside, aes(x, y, colour = id), alpha = 0.7, size = 3)+
  geom_spatvector(data=mtr, inherit.aes = FALSE, colour = "white", fill = NA)+
  scale_colour_viridis_d()+
  theme_void()
```

One real roost outside (Nutmeg's adventure as above) and 5 where birds were just roosting close to the fence. This is fine. 

Save output.

```{r}
# convert geometry to regular columns to save to disk
distance_roosts2 <- distance_roosts_latlon %>%
  st_drop_geometry() %>%
  mutate(longitude = st_coordinates(distance_roosts_latlon$geometry)[,1],
         latitude = st_coordinates(distance_roosts_latlon$geometry)[,2])

# plot on map without the far nutmeg point
ggmap(map_z15)+
  geom_path(data = filter(distance_roosts2, distance_previous<2000),
          aes(longitude, latitude, group = id), colour = "white", alpha = 0.6, inherit.aes = FALSE)+
    geom_point(data = filter(distance_roosts2, distance_previous<2000),
          aes(longitude, latitude, colour = distance_previous), inherit.aes = FALSE)+
  scale_colour_viridis_c()+
  theme_void()

# write to file
write.csv(distance_roosts2, "results/daily_distance_between_roosts.csv", row.names = FALSE)
```

## Release site fidelity

How far did they move from the release location? Based on daily roost distance from release location.

Calculate distance from daily roost to release location. 

Release location utm:(274423.45, 5801912.85) 
Release location lat/lon:(-37.902374, 144.434323)

```{r}
# read in roost locations, format as spatial
roosts <- read.csv("data/daily_roost_location.csv") %>%
  dplyr::select(!"npoints") %>%
  vect(geom = c("easting", "northing"), crs = "EPSG:32755") %>%
  st_as_sf()

# release coords as spatial
release  <- st_sfc(st_point(c(274423.45, 5801912.85) ), crs = 32755)

# calculate distance between roosts and release site
dist <- roosts %>%
  mutate(dist_release = as.numeric(st_distance(release, roosts))) %>%
  # keep max distance per day
  group_by(date, id) %>%
  filter(dist_release == max(dist_release)) %>%
  ungroup() %>%
  # add cohort information
  left_join(dplyr::select(metadata, c("cohort", "id"))) %>%
  # format date as a date
  mutate(date = as_date(date)) %>%
  # arrange by date
  arrange(date) %>%
  # drop geometry
  st_drop_geometry()

# save to file
write.csv(dist, "results/daily_roost_distance_from_release.csv", row.names = FALSE)
```

## Home range

Home range change is another way to investigate post-release behavioural modification. Previous studies have demonstrated decreasing home ranges over time as individuals first explore their new surrounds and then settle on a core area of exploitation. We define home range as 50% kernel utilisation distribution, with both nocturnal and diurnal fixes.

```{r}
# Read in cleaned data 
data <- read.csv("data/data_cleaned.csv") %>%
  # Time in posix format
  mutate(time_local = as.POSIXct(time_local, format = "%Y-%m-%d %H:%M:%OS", 
                                 tz = "Australia/Melbourne"),
         time_utc = as.POSIXct(time_utc, format = "%Y-%m-%d %H:%M:%OS", 
                                 tz = "UTC")) %>%
  # Add date
  mutate(date = as_date(time_local, tz = "Australia/Melbourne"))%>%
  filter(!id %in% c("Wobbles", "Star"))

# set up data in spatial points dataframe
locs <- SpatialPointsDataFrame(coordinates(
  cbind(data$easting, data$northing)), data = data)

# home range polygon per bird per day ðŸ¢ðŸ¢ 2.5 hours
birds <- as.character(unique(data$id))
days <- unique(locs[["date"]])

hr_daily <- data.frame()

for (i in 1:length(birds)){
  for (j in 1:length(days)){
    points <- subset(locs, id == birds[i] & date == days[j],
                     select = id)
    if (length(points)<5){
      next
    }
    kud <- kernelUD(points[,1], h="href", grid=1000, extent = 4) %>% 
      getverticeshr(percent = 50)
    
    proj4string(kud) <- CRS("EPSG:32755")
    
    kud_df_utm <- st_as_sf(kud)%>%
      mutate(date = days[j])
    
    print(head(kud_df_utm, n= 1L))
    
    hr_daily <- rbind.data.frame(hr_daily, kud_df_utm)
  }}

# extract area per day i.e. drop geometry
hr_area <- st_drop_geometry(hr_daily)

# save to file
write.csv(hr_area, "results/daily_hr_area.csv", row.names = TRUE)
```

## Time budget HMM

Time budget can be another indicator of post-release behavioural modification. We expect them to become more efficient with their foraging over time as they learn where to find and exploit resources. So we expect to see more direct travel and less time spent foraging over time. 

How do they allocate their nocturnal movement? Hidden markov model.

```{r}
# read in cleaned night data
data_night <- read.csv("data/data_clean_night.csv") %>%
  # drop columns notneeded
  dplyr::select(!c(sunrise, sunset, suntime, tod)) %>%
  # format date time in posixct
  mutate(time_local = as.POSIXct(time_local),
         date_bird = as_date(date_bird))

# prep data for moveHMM
data_hmm <- data_night %>%
  prepData(type = "UTM", coordNames = c("easting", "northing"),
           covNames = c("acceleration_raw_x", "acceleration_raw_y", "acceleration_raw_z", "altitude")) %>%
# add vector of the dynamic body acceleration (VEDBA)
  mutate(vedba = ((sqrt(acceleration_raw_x^2 + acceleration_raw_y^2 + acceleration_raw_x^2))/1000))
```

### Test model

Create a test subset of one bird for one month (Nutmeg in June 2023). Investigate step and angle statistics to help define starting parameters. 

```{r}
# Test subset, Nutmeg 1 month
dates <- seq(as.Date("2023-06-01"), as.Date("2023-07-01"), "days")
test <- filter(data_hmm, id == "Aurora" & date_bird %in% dates)

# format as amt and calculate step lengths
steps <- test %>%
  # format as amt track - add columns as needed
  make_track(.x = x, .y = y, .t = time_local, id = id, date_bird = date_bird)

# check sample rate
summarize_sampling_rate(steps)

# resample track to 5 minutes
track_resample(rate = minutes(5), tolerance = minutes(1))

# step statistics
hist(test$step, xlab = "step", main = "", breaks = 30)
summary(test$step)

# angle statistics
hist(test$angle, breaks = seq(-pi, pi, length = 15), xlab = "angle", main = "")
summary(test$angle)

```

#### Number of states

Compare a three state and four state model to see what performs better.

```{r}

# 3 state model parameters
stepPar <- c(
  10, 60, 200,   # Means
  5, 20, 150,    # Standard deviations 
  0.15, 0, 0.5   # Zero-mass values
)

anglePar <- c(
  0, 0, 0,      # Means
  1.5, 7, 3     # Concentrations
)

# 3 state model
hmm3 <- momentuHMM::fitHMM(data = test, 
               nbStates = 3,
               dist = list(step = "gamma", angle = "vm"),
               Par0 = list(step = stepPar, angle = anglePar),
               estAngleMean = list(angle=TRUE),
               formula = ~vedba
               )

hmm3
plot(hmm3)

# Step Parameters for 4 States
stepPar <- c(
  10, 60, 150, 300,   # Means 
  5, 15, 100, 150,    # Standard deviations 
  0.1, 0, 0.3, 0.15   # Zero-mass values 
)

# Angle Parameters for 4 States
anglePar <- c(
  0, 0, 0, 0,         # Means 
  1.5, 5, 2, 3        # Concentrations
)

# 4 state model
hmm4 <- momentuHMM::fitHMM(data = test, 
               nbStates = 4,
               dist = list(step = "gamma", angle = "vm"),
               Par0 = list(step = stepPar, angle = anglePar),
               estAngleMean = list(angle=TRUE),
               formula = ~vedba
               )

hmm4
plot(hmm4)

# compare 3 and 4 state models
AIC(hmm3, hmm4)
```

Prefer the 4 state model both visually and on AIC.

Model      AIC
1  hmm4 19401.37
2  hmm3 19443.89

#### Optimise parameters

Try a range of starting values to optimise parameters.

```{r}
# For reproducibility
set.seed(12345)

# Number of tries with different starting values
niter <- 10

# Save list of fitted models
allm <- list()

# Save list of starting parameters
parameters <- list()

for(i in 1:niter) {
  # Step length mean
  stepMean0 <- runif(4, min = c(5, 50, 100, 200), max = c(50, 150, 300, 400))
  
  # Step length standard deviation
  stepSD0 <- runif(4, min = c(5, 30, 50, 100), max = c(30, 100, 150, 200))
  
  # Zero mass
  zeroMass0 <- runif(4, min = c(0.5, 0.05, 0.01, 0.2), max = c(0.9, 0.3, 0.1, 0.3))
  
  # Turning angle mean
  angleMean0 <- c(0, 0, 0, 0)
  
  # Turning angle concentration
  angleCon0 <- runif(4, min = c(0.5, 3, 5, 3), max = c(3, 10, 15, 10))
  
  # Parameter table
  parameters[[i]] <- data.frame(
    state = rep(1:4),
    stepMean0 = stepMean0,
    stepSD0 = stepSD0,
    zeroMass0 = zeroMass0,
    angleMean0 = angleMean0,
    angleCon0 = angleCon0
  )
  
  # Fit model
  stepPar0 <- c(stepMean0, stepSD0, zeroMass0)
  anglePar0 <- c(angleMean0, angleCon0)
  
  allm[[i]] <- tryCatch({
    momentuHMM::fitHMM(
      data = test,
      nbStates = 4,
      dist = list(step = "gamma", angle = "vm"),
      Par0 = list(step = stepPar0, angle = anglePar0),
      estAngleMean = list(angle = TRUE),
      formula = ~vedba
    )
  }, error = function(e) NULL)
}

# Extract likelihoods of fitted models
allnllk <- unlist(lapply(allm, function(m) m$mod$minimum))

# Index of best fitting model (smallest negative log-likelihood)
whichbest <- which.min(allnllk)

# Best fitting model
mbest <- allm[[whichbest]]
mbest
plot(mbest)
plotStates(mbest)

# Best starting parameters 
pbest_start <- parameters[[whichbest]]

# Best outcome parameters
pbest_outcome <- mbest$mle
```

Best outcome parameters:

$step
           state 1    state 2     state 3     state 4
mean     7.2211441 28.7419041 93.49649922 253.7447067
sd       5.6455927 23.2401216 53.89146576 312.9671872
zeromass 0.1559012  0.1669094  0.01833069   0.2101607

$angle
                state 1    state 2    state 3   state 4
mean          0.1170206 -0.1668971 0.02669791 0.2645331
concentration 1.1835577  1.3724136 2.71667136 1.9204624

$beta
               1 -> 2    1 -> 3    1 -> 4    2 -> 1     2 -> 3    2 -> 4
(Intercept) -5.204991 -8.829121 -2.213485 -6.221311 -10.254742 -4.297407
vedba        6.313020 -3.078513 -1.617744  5.398456  -4.197239  4.724587
               3 -> 1    3 -> 2     3 -> 4    4 -> 1    4 -> 2    4 -> 3
(Intercept) -8.220560 -2.225103  0.4526661 -2.324431 -0.453848  1.538068
vedba       -3.345948  1.189772 -2.1410536  1.063941  1.259101 -3.388000

$delta
               state 1   state 2      state 3      state 4
ID:Nutmeg 0.0004158271 0.9993644 5.228222e-05 0.0001674458

### Run HMM

Apply best parameters for a 4 state model to all birds.

```{r}
# Step Parameters 
stepPar <- c(
  7, 29, 93, 253,   # Means
  6, 23, 54, 313,    # Standard deviations 
  0.16, 0.17, 0.02, 0.21   # Zero-mass values 
)

# Angle Parameters for 4 States
anglePar <- c(
  0.11, -0.17, 0.03, 0.26,         # Means 
  1.18, 1.37, 2.72, 1.92        # Concentrations
)

# 4 state model (approx. 7 hours to run) ðŸ¢ðŸ¢
hmm_all <- momentuHMM::fitHMM(data = data_hmm, 
               nbStates = 4,
               dist = list(step = "gamma", angle = "vm"),
               Par0 = list(step = stepPar, angle = anglePar),
               estAngleMean = list(angle=TRUE),
               formula = ~vedba
               )

# see states
hmm_all

# plot results
plot(hmm_all)

# check residuals
plotPR(hmm_all)

# Decode states
data_hmm2 <- data_hmm %>%
  mutate(states = viterbi(hmm_all))

# save output
write.csv(data_hmm2, "data/data_hmm.csv", row.names = FALSE)

```

$step
$step$est
           state 1    state 2    state 3     state 4
mean     8.2208094 20.3895440 57.8629585 129.4745424
sd       6.6718586 17.2595917 33.1696834 142.4877739
zeromass 0.1835534  0.1435767  0.1017199   0.1056076

$step$se
             state 1     state 2     state 3    state 4
mean     0.059195569 0.158752114 0.504588470 1.62585187
sd       0.059392166 0.135826249 0.350108882 1.68770684
zeromass 0.001644487 0.001121659 0.002745764 0.00326152

$step$lower
           state 1    state 2     state 3      state 4
mean     8.1047882 20.0783956 56.87398323 126.28793127
sd       6.5554521 16.9933771 32.48348260 139.17992928
zeromass 0.1803303  0.1413783  0.09633832   0.09921516

$step$upper
           state 1    state 2    state 3     state 4
mean     8.3368306 20.7006924 58.8519337 132.6611535
sd       6.7882651 17.5258063 33.8558842 145.7956185
zeromass 0.1867766  0.1457751  0.1071015   0.1120001


$angle
$angle$est
                    state 1     state 2     state 3    state 4
mean          -0.0006350116 -0.02490599 0.005901283 0.02889899
concentration  0.9849618919  1.34078324 2.855987790 1.62310950

$angle$se
                  state 1     state 2     state 3    state 4
mean          0.008772497 0.004360951 0.005746417 0.01000978
concentration 0.009074356 0.008098515 0.040103562 0.01989011

$angle$lower
                  state 1    state 2      state 3     state 4
mean          -0.01782879 -0.0334533 -0.005361488 0.009280183
concentration  0.96717648  1.3249104  2.777386254 1.584125600

$angle$upper
                 state 1     state 2    state 3    state 4
mean          0.01655877 -0.01635868 0.01716405 0.04851779
concentration 1.00274730  1.35665604 2.93458933 1.66209339


$gamma
$gamma$est
            state 1    state 2      state 3    state 4
state 1 0.931458331 0.05761075 2.899808e-05 0.01090192
state 2 0.025798292 0.89292770 4.665911e-02 0.03461490
state 3 0.002059385 0.20857863 7.299991e-01 0.05936288
state 4 0.036632497 0.16340398 1.137106e-01 0.68625289

$gamma$se
            state 1     state 2     state 3      state 4
state 1 0.001852868 0.001856780         NaN 0.0009593681
state 2 0.001035936 0.001734979 0.001537815 0.0010952092
state 3 0.001406860 0.004757413 0.005098452 0.0034170704
state 4 0.002882481 0.006547791 0.005798381 0.0077365855

$gamma$lower
            state 1    state 2    state 3     state 4
state 1 0.927736437 0.05407762        NaN 0.009173398
state 2 0.023843831 0.88947937 0.04373596 0.032531334
state 3 0.000539143 0.19940768 0.71989062 0.053008962
state 4 0.031383508 0.15097250 0.10283144 0.670894349

$gamma$upper
            state 1    state 2    state 3    state 4
state 1 0.935001961 0.06135974        NaN 0.01295190
state 2 0.027908379 0.89628098 0.04976747 0.03682684
state 3 0.007832716 0.21805649 0.73987462 0.06642498
state 4 0.042720676 0.17664613 0.12557970 0.70121389

### Time budget

```{r}
# read in data with hmm states
data_hmm <- read.csv("data/data_hmm.csv")

# calculate proportion of fixes in each state per bird per day
budget_daily <- data_hmm %>%
  group_by(ID, date_bird) %>%
  # count of states
  count(states) %>%
  # pivot to wide format
  pivot_wider(names_from = 3, values_from = 4) %>%
  # rename state columns
  rename(state1 = 3,
         state2 = 4,
         state3 = 5,
         state4 = 6) %>%
  # replace NAs with zeros
  replace(is.na(.), 0) %>%
  # calculate proportion
  mutate(total = sum(state1, state2, state3, state4),
         state1p = state1/total,
         state2p = state2/total,
         state3p = state3/total,
         state4p = state4/total) %>%
  arrange(date_bird) %>%
  # rename ID
  rename(id = ID)

# save to file
write.csv(budget_daily, "results/daily_time_budget.csv", row.names = FALSE)

# summarise over time
budget <- budget_daily %>%
  # calculate mean and sd
  group_by(id) %>%
  summarise(s1m = mean(state1p),
            s2m = mean(state2p),
            s3m = mean(state3p),
            s4m = mean(state4p)) %>%
  pivot_longer(2:5, names_to = "state", values_to = "budget") %>%
  # add cohort
  left_join(select(metadata, c(id, cohort)))

# plot time budget mean by bird
ggplot(budget)+
  geom_bar(aes(x = id, y = budget, fill = state), position="fill", stat="identity")+
  theme_minimal()+
  scale_fill_viridis_d()+
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        axis.ticks.x=element_blank(),
        axis.text.x = element_text(angle = 90, vjust = 0, hjust = 1))+
  facet_wrap(~cohort, scales="free_x")
```

# Conspecific interactions

The key issue addressed in this paper is whether social interaction occurs between residents and reinforcers, and if so whether this mediates post-release outcomes for the cohorts. 

## Proximity anaysis

First we need to determine interactions. Using GPS tracking data we cannot perfectly capture social interactions, but we can approximate these with proximity (distance and time) analysis. I elected to use the coefficient of association because it produces few few I errors, owing to its simplicity ([Long et al 2014](https://besjournals.onlinelibrary.wiley.com/doi/10.1111/1365-2656.12198)) 

To determine the temporal interaction threshold we use the mean sampling rate.

```{r}
# Read in cleaned data 
data <- read.csv("data/data_cleaned.csv") %>%
  # Time in posix format
  mutate(time_local = as.POSIXct(time_local, format = "%Y-%m-%d %H:%M:%OS", 
                                 tz = "Australia/Melbourne")) %>%
  # Add date
  mutate(date = as_date(time_local, tz = "Australia/Melbourne")) %>%
  # drop birds excluded from analyses
  filter(!id %in% c("Wobbles", "Star")) %>%
  # drop NAs
  na.omit()

# summarise sampling rate to decide on temporal interaction threshold
steps <- data %>%
  make_track(.x = easting, .y = northing, .t = time_local, id = id) %>%
  nest(data = -"id")

sampling_rates <- steps %>%
  transmute(id, summary = map(data, summarize_sampling_rate)) %>%
  unnest(summary)

mean(sampling_rates$mean)
```

The mean sampling rate for the cleaned data is 8 minutes. We will use a time parameter of 4 minutes (half the sampling interval) following advice from the wildlifeDI package vignette. 

First format tracks as move2 objects for use in wildlifeDI.

```{r}
# format track data as move2 object
move <- mt_as_move2(data, coords = c("easting", "northing"), time_column = "time_local", track_id_column = "id") %>%
  # add crs
  sf::st_set_crs("EPSG:32755") %>%
  # needed to omit NAs for wildlifeDI to work
  na.omit()
```

We use a dcPlot of paired encounters across distance to approximate the distance threshold (using a subset to reduce processing time).

Our practitioner guesstimate was a threshold of 30m (based on visual observation of interactions between individuals in the field during monitoring). 

```{r}
# one week subset
temp <- seq.Date(from = as_date("2023-06-01"), to = as_date("2023-06-07"), by = "day")
move_subset <- filter(move, date %in% temp)

# plot distribution of paired fixes by distance
dcPlot(move_subset,tc=4*60,dmax=500)
dcPlot(move_subset,tc=4*60,dmax=100)
```

The majority of contacts took place <40m apart. I'll retain the guesstimate of 30m as the parameter. This is also within tolerance for the approximate GPS uncertainty of approx 20m (on cleaned data).

We calculate prox for all dyads for the two time periods of interest:

  1) Residents only
  2) Residents and reinforcers
  
### Residents only

We define this as the period from the last resident released (2023-01-16) to the first reinforcer released (2023-06-07). 

```{r}
# define resident only date period
res_dates <- seq.Date(from = as_date("2023-01-16"), to = as_date("2023-06-06"), by = "day")

# subset to dates
data_resident <- move %>%
  filter(date %in% res_dates)

# Test for a single dyad
dyad <- filter(data_resident, id %in% c("Aurora", "Briar"))

checkTO(dyad)

test <- Ca(dyad, tc=4*60, dc=30);Sys.time()
```

The test dyad took 47 seconds so I'm anticipating the full set of 78 interactions will take 1 hour. 

```{r}
# list of residents
residents <- unique(data_resident$id)

# All bird combinations
list <-combinations(n = 13, r = 2, v = residents, repeats.allowed = FALSE)
list1 <- list[,1]
list2 <- list[,2]

# Calculate interactions between all birds ðŸ¢ðŸ¢
interact <- data.frame()

for(i in 1:length(list1)) {
  
  dyad <- filter(data_resident, id %in% c(list1[i], list2[i]))
  
  temp <- tryCatch({
    data.frame(Prox(dyad, tc=4*60, dc=30)) %>%
      clean_names()
  }, error = function(e) data.frame(ca = NA, bird1 = NA, bird2 = NA)) 
  
  print(paste("Finished", list1[i], "&", list2[i], i, "of 78", sep = " "))
  
  interact <- rbind(interact, temp)
}

# tidy up the output
interact2 <- interact %>%
  # remove unneeded columns
  select(id1, id2, prox) %>%
  # format prox numeric to 4 decimal places
  mutate(prox = as.numeric(format(round(prox, 4), nsmall = 4)))

# save to disk
write.csv(interact, "results/proximity_residents.csv", row.names = FALSE)

# summary statistics prox values
hist(interact2$prox)
summary(interact2$prox)
```

Mean proximity score is 13.9%, min is no interaction and max is 72.4%. 

### Residents and reinforcers

We define this as the period from the last reinforcer released (2023-06-11) to the end of the study period (when reinforcers moved to Orana 2023-08-05). Each dyad takes approx. 8 seconds so all 378 takes approx 1 hour to run. 

```{r}
# define reinforcer date period
rein_dates <- seq.Date(from = as_date("2023-06-11"), to = as_date("2023-08-05"), by = "day")

# subset to dates
data_reinforce <- move %>%
  filter(date %in% rein_dates)

# list of founders
founders <- unique(data_reinforce$id)

# All bird combinations
list <-combinations(n = 28, r = 2, v = founders, repeats.allowed = FALSE)
list1 <- list[,1]
list2 <- list[,2]

# Calculate interactions between all birds ðŸ¢ðŸ¢
interact <- data.frame()

for(i in 1:length(list1)) {
  
  dyad <- filter(data_reinforce, id %in% c(list1[i], list2[i]))
  
  temp <- tryCatch({
    data.frame(Prox(dyad, tc=4*60, dc=30)) %>%
      clean_names()
  }, error = function(e) data.frame(ca = NA, bird1 = NA, bird2 = NA)) 
  
  print(paste("Finished", list1[i], "&", list2[i], i, "of 378", sep = " "))
  
  interact <- rbind(interact, temp)
}

# tidy up the output
interact2 <- interact %>%
  # remove unneeded columns
  select(id1, id2, prox) %>%
  # format prox numeric to 4 decimal places
  mutate(prox = as.numeric(format(round(prox, 4), nsmall = 4)))

# save to disk
write.csv(interact2, "results/proximity_reinforcers.csv", row.names = FALSE)

# summary statistics prox values
hist(interact2$prox)
summary(interact2$prox)
```

Summary stats here

## Social network

For social network analysis we follow [Farine & Whitehead 2015](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/1365-2656.12418) where the edges are the proximity metrics calculated above. Proximity networks are inferred interactions but with a highly complete data set - so trade-off temporal resolution with little information on any one interaction. 

We use the packages tidygraph for network diagrams and asnipe to implement animal social network analysis.

### Residents only

```{r}
# read in dyad proximity scores
prox <- read.csv("results/prox_residents.csv") %>%
  # select id and prox columns
  select(id1, id2, prox) %>%
  # format prox numeric to 3 dec places
  mutate(prox = as.numeric(format(round(prox, 4), nsmall = 4)))


```



### Residents and reinforcers

# Statistical analysis

We hypothesised members of the mixed resident-reinforcer social group would proceed more quickly through the PRBM phases than those in the reinforcer only group. I also want to compare both reinforcer groups to the residents to see if one is more similar than the other.

```{r}
# social group membership
club <- c("Rowan", "Rory", "Fauna", "Zeus", "Rocky", "Sofi", "Loki", "Avery", "Maeve")

# define reinforcer date period
rein_dates <- seq.Date(from = as_date("2023-06-11"), to = as_date("2024-01-10"), by = "day")
```

### m1: Distance moved

```{r}
# read in data and filter to reinforcement period
datam6 <- read.csv("results/daily_distance_moved.csv") %>%
  # add metadata
  left_join(dplyr::select(metadata, c("id", "cohort"))) %>%
  # format dates as dates
  mutate(date = as_date(date_bird)) %>%
  # add column for membership to social group
  mutate(social = ifelse(id %in% club, "exclusive", ifelse(cohort == "Resident", "mixed-resident", "mixed-reinforcer"))) %>%
  # limit to reinforcement period
  filter(date %in% rein_dates)

# create social group means
sum6 <- datam6 %>%
  group_by(date, social) %>%
  summarise(daily_dist = mean(daily_dist)) %>%
  arrange(date)

# plot daily distance by social group
ggplot(datam6)+
  geom_boxplot(aes(social,daily_dist))+
  theme_minimal()+
  xlab("Social group") + 
  ylab("Daily distance moved (m)")

# plot daily distance over time as social group means
ggplot(sum6)+
  geom_path(aes(date, daily_dist, color = social))+
  theme_minimal() +
  xlab("Days post-release") + 
  ylab("Daily distance moved (m)")

# plot daily distance over time as linear model
ggplot(datam6)+
  geom_smooth(aes(date, daily_dist, color = social), method = "lm")+
  theme_minimal() +
  xlab("Days post-release") + 
  ylab("Daily distance moved (m)")
```


```{r}
# distribution of movement data
hist(datam6$daily_dist)
ggqqplot(datam6$daily_dist)

# correct left skew
hist(sqrt(datam6$daily_dist))
ggqqplot(sqrt(datam6$daily_dist))

# test whether establishment differs between the groups
m6 <- lmer(sqrt(daily_dist) ~ social * scale(date) + (1|id), data = datam6)
summary(m6)
emmeans(m6, pairwise ~ social, adjust = "tukey", type = "response")

# check residuals
hist(residuals(m6))
ggqqplot(residuals(m6))
```

### m7: Roost establishment

```{r}
# read in data and filter to reinforcement period
datam7 <- read.csv("results/daily_distance_between_roosts.csv") %>%
  # add metadata
  left_join(dplyr::select(metadata, c("id", "cohort"))) %>%
  # format dates as dates
  mutate(date = as_date(date)) %>%
  # add column for membership to social group
  mutate(social = ifelse(id %in% club, "exclusive", ifelse(cohort == "Resident", "mixed-resident", "mixed-reinforcer"))) %>%
  # limit to reinforcement period
  filter(date %in% rein_dates)

# create cohort means
sum7 <- datam7 %>%
  group_by(date, social) %>%
  summarise(distance_previous = mean(distance_previous)) %>%
  arrange(date)

# plot daily roost distance by social group
ggplot(datam7)+
  geom_boxplot(aes(social,distance_previous))+
  theme_minimal()+
  xlab("Social group") + 
  ylab("Daily distance between roosts (m)")

# plot daily roost distance over time as social group means
ggplot(sum7)+
  geom_path(aes(date, distance_previous, color = social))+
  theme_minimal() +
  xlab("Date") + 
  ylab("Daily distance between roosts (m)")

# plot daily roost distance over time as linear model
ggplot(datam7)+
  geom_smooth(aes(date, distance_previous, color = social), method = "lm")+
  theme_minimal() +
  xlab("Date") + 
  ylab("Daily distance between roosts (m)")
```

The mixed group converges with the residents, whereas the exclusive group continues to increase.

```{r}
# distribution of movement data
hist(datam7$distance_previous)
ggqqplot(datam7$distance_previous)

# correct left skew
hist(log(datam7$distance_previous))
ggqqplot(log(datam7$distance_previous))

# test whether establishment differs between the groups
m7 <- lmer(log(distance_previous) ~ social * scale(date) + (1|id), data = datam7)
summary(m7)
emmeans(m7, pairwise ~ social, adjust = "tukey", type = "response")

# check residuals
hist(residuals(m7))
ggqqplot(residuals(m7))
```

Both mixed-reinforcer and mixed-resident groups exhibit lower roost displacements compared to the exclusive group. The magnitude of the effect is more pronounced in the mixed-resident group, but both comparisons are statistically significant.

### m8: Release site fidelity

Does release site fidelity differ between the social groups?

```{r}
# read in data and filter to reinforcement period
datam8 <- read.csv("results/daily_roost_distance_from_release.csv") %>%
  # add metadata
  left_join(dplyr::select(metadata, c("id", "cohort"))) %>%
  # format dates as dates
  mutate(date = as_date(date)) %>%
  # add column for membership to social group
  mutate(social = ifelse(id %in% club, "exclusive", ifelse(cohort == "Resident", "mixed-resident", "mixed-reinforcer"))) %>%
  # limit to reinforcement period
  filter(date %in% rein_dates)

# create cohort means
sum8 <- datam8 %>%
  group_by(date, social) %>%
  summarise(dist_release = mean(dist_release)) %>%
  arrange(date)

# plot release site distance by social group
ggplot(datam8)+
  geom_boxplot(aes(social, dist_release))+
  theme_minimal()+
  xlab("Social group") + 
  ylab("Distance from release site (m)")

# plot release site distance over time as social group means
ggplot(sum8)+
  geom_path(aes(date, dist_release, color = social))+
  theme_minimal() +
  xlab("Date") + 
  ylab("Distance from release site (m)")

# plot release site distance over time as linear model
ggplot(datam8)+
  geom_smooth(aes(date, dist_release, color = social), method = "lm")+
  theme_minimal() +
  xlab("Date") + 
  ylab("Distance from release site (m)")
```

The reinforcers in the mixed group stayed similarly close to the release site as the residents (who were occupying an area of the woodland near the release site when the reinforcement translocation occurred), whereas the exclusive group left the area.

```{r}
# distribution of movement data
hist(datam8$dist_release)
ggqqplot(datam8$dist_release)

# correct left skew - still not great but a bit better
hist(sqrt(datam8$dist_release))
ggqqplot(sqrt(datam8$dist_release))

# test whether establishment differs between the social groups
m8 <- lmer(sqrt(dist_release) ~ social * scale(date) + (1|id), data = datam8)
summary(m8)
emmeans(m8, pairwise ~ social, adjust = "tukey", type = "response")

# check residuals
hist(residuals(m8))
ggqqplot(residuals(m8))
```

Both mixed-reinforcer and mixed-resident groups have significantly lower dist_release compared to the exclusive group.

Significant negative interactions between social group and date for both mixed-reinforcer (-3.0415, p < 0.00001) and mixed-resident (-2.6524, p < 0.00001) groups, indicating that the increase in dist_release over time is less pronounced for these groups compared to the exclusive group.

### m9: Home range establishment

### m10: Time budget

### m11: Weight change

In addition to the movement-based metrics of performance I wanted to look at health check data. Weight change post-release is a good indicator of performance, where some loss is expected but less steep decline (trending towards stability) indicates better performance.

We were unable to compare residents to reinforcers for weight change because we did not have weight measurements for the six individuals released in January 2023 (backpacks fitted in the December trip and released by Mt Rothwell staff rather than me going back down there). So we can only compare reinforcers, but interested to see how the social group memberships contrast.

Unlike the movement data, the study period is for 70 days because weights were taken after the 55 minimum common tracking period. 

```{r}
# read in data and clean up
datam11 <- read_xlsx("data/healthchecks.xlsx") %>%
  clean_names() %>%
  rename(id = identity) %>%
  select(id, date, weight) %>%
  mutate(date = as_date(date)) %>%
  # add metadata
  left_join(select(metadata, c(id, start_date, cohort))) %>%
  drop_na() %>%
  # calculate elapsed time and add status
  mutate(elapsed = as.numeric(date - start_date)) %>%
  # keep only reinforcers
  filter(cohort == "Reinforcing") %>%
  # add social group membership %>%
   mutate(social = ifelse(id %in% club, "exclusive", ifelse(cohort == "Resident", "mixed-resident", "mixed-reinforcer"))) %>%
  # keep only the latest pre-release values and assign to pre/post release
  group_by(id) %>%
  filter(elapsed > 0 | elapsed == max(elapsed[elapsed <= 0])) %>%
  ungroup() %>%
  mutate(status = as_factor(ifelse(elapsed > 0, "post-release", "pre-release"))) %>%
  arrange(id) %>%
  # filter to study period
  filter(elapsed <70)

# plot weight pre and post-release
ggplot(datam11)+
  geom_boxplot(aes(status, weight))+
  theme_minimal()

# plot weight pre and post release by individual, facet wrap social
ggplot(datam11)+
  geom_path(aes(status, weight, colour = id, group = id))+
  theme_minimal()+
  scale_colour_viridis_d()+
  facet_wrap(~social)

# plot weight pre and post release by social group as a linear model
ggplot(datam11)+
  geom_smooth(aes(status, weight, colour = social, group = social), method = "lm")+
  theme_minimal()
```

We have pre and post data for 18 of the 20 reinforcers. All lost weight. The mixed social group declined less steeply, but started out on average lower than the exclusive group.

Next I calculated the change in weight both in grams and as a proportion of starting weight (to account for variation in body sizes).

```{r}
# calculate weight change in grams and as a proportion of starting weight
sum11 <- datam11 %>%
  select(c(id, social, weight, status)) %>%
  pivot_wider(names_from = status, values_from = weight) %>%
  clean_names() %>%
  drop_na() %>%
  # add change columns
  mutate(diff = pre_release - post_release,
         proportion = diff/pre_release)

# plot total weight differences by social group
ggplot(sum11)+
  geom_boxplot(aes(social, diff))+
  theme_minimal()

# plot proportional weight differences by social group
ggplot(sum11)+
  geom_boxplot(aes(social, proportion))+
  theme_minimal()
```

Although total weight change seems on average lower in the mixed group, proportionally it does not look like a lot of difference.

```{r}
# check distribution of the difference
hist(sum11$diff)
ggqqplot(sum11$diff)

# summary stats
sums <- sum11 %>%
  group_by(social) %>%
  summarise(mean_diff = mean(diff),
            se_diff = sd(diff, na.rm = TRUE) / sqrt(n()),
            mean_prop = mean(proportion),
            se_prop = sd(proportion, na.rm = TRUE) / sqrt(n()))

# test whether weight change differs between the social groups
m11 <- lm(diff ~ social, data = sum11)
summary(m11)
emmeans(m8, pairwise ~ social, adjust = "tukey", type = "response")
```

There is no significant differences between groups for weight loss.

## Effect on residents pre- and post-reinforcement

# Survival analysis

## Reinforcers and residents

Do reinforcers survive better than the residents? 

This is hard to answer because not tracked as long and some birds were subsequently moved to Orana (right censored data). Attempt to answer this using Kaplan Meier Analysis

Start by looking at the overall survival probabilities for all birds.

```{r m6}
# Format data for Kaplan Meier Analysis - where death is TRUE i.e. 1
surv_data <- metadata %>%
  # remove columns not needed
  dplyr::select(!c(location, abbbs, band, alive, release_year)) %>%
  # add survival time (persistence) - right censored data
  mutate(time = end_date - start_date) %>%
  # add status - whether death occurs
  mutate(status = ifelse(is.na(mortality), 0, 1))

km <- with(surv_data, Surv(time, status))
  
# Fit a basic survival model
kmfit <- survfit(Surv(time, status) ~ 1, data = surv_data)

# Estimated survival probabilities
summary(kmfit, times = c(1, 10, 30, 55, 100, 365))

# plot
autoplot(kmfit)
```

The probability of survival to 1 year is 50% (CI: 34-75%). The probability of survival to 55 days (the study period) is 91% (CI: 83-100%).

Next we compare survial by cohorts.

```{r}
# Fit a survival model by cohort
kmfit_cohort <- survfit(Surv(time, status) ~ cohort, data = surv_data)

summary(kmfit_cohort, times = c(1,30,55))

# plot
autoplot(kmfit_cohort)

# Fit Cox Model
cox <- coxph(Surv(time, status) ~ cohort, data = surv_data)
summary(cox)
```

The probability of survival to 55 days for residents is 87% (CI: 71-100%) and for reinforcers is 95% (CI: 86-100%). So reinforcers are slightly more likely to survive, however this effect is not significant (p=0.754). There were two disease events (one in each cohort) that accounted for most of the early deaths. Potentially the stress of translocation caused these individuals to succumb to illness. 

## Reinforcers by social group

During the social analysis step we found that post-reinforcement there were two sub-groups to the community:

  1) a group with just reinforcers, and 
  2) a blended group of all remaining residents and half the reinforcers.
  
I hypothesise the group with the residents would survive better because of the enhanced potential for learning. 

Compare to 55 days (minimum common denominator deployment period for reinforcers).

```{r}
# social group membership
club <- c("Rowan", "Rory", "Fauna", "Zeus", "Rocky", "Sofi", "Loki", "Avery", "Maeve")

# Format data for Kaplan Meier Analysis - where death is TRUE i.e. 1
surv_data <- metadata %>%
  # remove columns not needed
  dplyr::select(!c(location, abbbs, band, alive, release_year)) %>%
  # add survival time (persistence) - right censored data
  mutate(time = end_date - start_date) %>%
  # add status - whether death occurs
  mutate(status = ifelse(is.na(mortality), 0, 1)) %>%
  # add column for membership to social group
  mutate(social = ifelse(id %in% club, "exclusive", "mixed")) %>%
  # select only reinforcers
  filter(cohort == "Reinforcing")

# Fit a survival model by social group
kmfit_social <- survfit(Surv(time, status) ~ social, data = surv_data)

summary(kmfit_social, times = c(1,30,54))

# plot
autoplot(kmfit_social)

# Fit Cox Model
cox <- coxph(Surv(time, status) ~ social, data = surv_data)
summary(cox)
```

No difference in survival for reinforcers of different social memberships.
